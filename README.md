## Trustworthy AI in Medical Imaging 

This project aims at fostering an inter-disciplinary discussion on Trustworthy AI in Medical Imaging. The tutorial features a panel of experts in medical imaging, AI, cryptography, medicine, law and ethics, which will illustrate and discuss the current state-of-the-art and challenges for the development of Trustworthy AI in healthcare.
Trustworthiness is rapidly becoming a requirement of paramount importance for any real-life deployment of artificial intelligence (AI). 

The recent EU High-level expert group on artificial intelligence  identifies the following key components of Trustworthy AI: technical robustness, safety, accuracy, privacy, data governance, explainability and bias assessment.

The problem of Trustworthy AI in medical imaging raises a broad range of technical and societal questions, which require a strong interplay between communities to match methodological and technical advances with a better understanding of the ethical and legal implications of the deployment of IA in real life. 


Proposed schedule:

-	Introduction
-	Part 1: Privacy Preserving Machine Learning 
The goal of Privacy Preserving Machine Learning (PPML) is to identify customized algorithms that would, by design, preserve the privacy of the processed data. Fully homomorphic encryption or secure multi-party computation are popular cryptographic techniques for PPML. Yet, these often incur high computational and/or communication costs. In this talk, we will analyse the tension between ML techniques and relevant cryptographic tools, and overview existing solutions addressing privacy requirements.
-	Part 2: Technical Robustness and Bias in Medical Imaging
Technical robustness refers to the capacity of AI systems to be adverse to risks and to behave reliably, minimising and preventing unintentional and unexpected harm. This talk will cover three of the key points that robust AI systems should address: 1) safety and fall-back plans; 2) accuracy; and 3) reliability and reproducibility. We will discuss to which extent these are being address in current medical imaging applications, and conclude with an overview of the effect of bias on a system’s robustness, and related mitigation strategies. 
-	Part 3: Federated Learning for Data Governance in Collaborative AI
Federated Learning (FL) is an attractive Data Governance solution in healthcare to avoid sharing raw data between hospitals. Applying FL to real-life is however challenging, due to the sensitivity to data heterogeneity, or the lack of standards for robust and safe FL infrastructures. This talk will cover the state of the art and principles of FL, with current applications in healthcare. We will discuss measures to mitigate the problem of bias and security, and analyse current open-source initiatives bringing FL in hospitals. 
- Part 4: Law and Ethics of Trustworthy AI in Medical Applications.
Adopting AI in healthcare requires the trust of all users and, above all, respect for the social acceptability of algorithms. The is a risk of perpetuating existing societal biases through biases in data and algorithms, and the opacity of complex AI processes. The legal system has a major role in building the confidence of all users and ensuring informed consent. Raising public awareness on AI in healthcare and understanding its implications are essential to ensure transparency.  An ethical debate on AI in healthcare becomes critical.
-	Part 5: Clinical Use Case: Federated Learning in Multi-Centric Oncology Studies
This talk will present  a French Federated Learning initiative for the collaborative analysis of 3D PET/CT images and clinical data in oncology. This FL use-case deploys Fed-BioMed , an open-source FL framework, in a real-word healthcare scenario composed by a network of 10 hospitals. The interdisciplinary nature of the project requires an efficient collaboration between different actors: researchers, engineers, medical doctors, and hospitals’ IT System Departments.
-	Wrap-up & Conclusion

